{"/":{"title":"","content":"# Hello World\nHello! This is my first, and hopefully only, Zettelkasten. It's a tool I plan to use to create hubs of knowledge in my mind so I can gravitate away from the mundane structure of normal note-taking. I have no clue what it will look like at the time you are reading this, but hopefully I can grow it from the dark, empty canvas I see right now to something comprehensive and beautiful. \n\nWith that in mind... welcome! I hope you get the same joy reading this as I do making it!\n\nSome of my interests:\n- [Artificial Intelligence](Artificial%20Intelligence.md), 06/12/2022\n\nEnjoy!","lastmodified":"2022-06-14T23:22:48.537194594Z","tags":null},"/notes/Artificial-Intelligence":{"title":"","content":"# Artificial Intelligence\nOne of my favorite topics to learn about. The whole concept of machines using the human development of knowledge creates a wonderful connection between biology and technology, 2 unalike fields. There are lots of misconceptions regarding this concept, so the proper definition of AI is simply *the connection between algorithms and datasets to establish patterns.*\n\n**Hierarchy**:\nArtificial Intelligence -\u003e [Machine Learning](Machine%20Learning.md) -\u003e [Representation Learning](Representation%20Learning.md) -\u003e [Deep Learning](Deep%20Learning.md) \n## Quick Recap\nBasically, early successes of AI were in very rudimentary settings (IBM AI winning chess championships). But that didn't really mean much, since the rules of those environments were very simply, not like the real world.\n\nSome people tried to hard code the rules of the world into an AI, called the Knowledge Base Approach -\u003e wasn't effective -\u003e shows that machines need to learn knowledge through patterns from raw data\n## Answer\n[Machine Learning](Machine%20Learning.md)! It successfully yielded \"subjective\" results that were more attuned to reality\n\nFurther reading: Deep Learning textbook","lastmodified":"2022-06-14T23:22:48.537194594Z","tags":null},"/notes/Auto-Encoders":{"title":"","content":"# Auto Encoders\nAn important, iconic system in AI classification that involves sending an input through some sort of conversion/encryption tool, then decrypting the result to match the original input as close as possible. The system is fed data of correct conversions, and the system learns the rules of the conversion, and can slowly decipher the code into something similar to the input. Some applications of this technology involves detecting anomalies in datasets, since the expected output would differ highly from the AI output.","lastmodified":"2022-06-14T23:22:48.537194594Z","tags":null},"/notes/Deep-Learning":{"title":"","content":"# Deep Learning\nRepresentations in terms of more representations (inception type vibe)\n\n### Example\nSay you took a selfie, and we wanted an AI to find you in the picture. To break it down, we will first take the most basic representation, then make another representation in terms of that until we get to you.\n\nPixels -\u003e Edges -\u003e Contours -\u003e Objects -\u003e Person\n\nA fantastic example of this is the [Multi-Layer Perceptron](Multi-Layer%20Perceptron.md), which deserves its own page. But TLDR, it maps input values onto output values, and is a math function composed of many smaller functions, aka representations.\n\n## Measuring Depth\n1. The number of sequential instructions needed to evaluate the system architecture (the absolute longest path through the flow chart to get from input to output). [Pasted image 20220613021511](Pasted%20image%2020220613021511.png)\n2. Depth of graph describing relationship between **concepts**. It will likely be smaller than the first method. \n\nComparison of measuring depth with AI observing face of person with a shadow covering half their face.\n\nLayers | Method 1 | Method 2\n------------ | ------------ | ------------\n1 |  1 eye visible | eyes\n2 | portion of face visible | faces\n3 | infers 2nd eye exists | \n\n### Quick summary of process to get to Deep Learning\n- **[Artificial Intelligence](Artificial%20Intelligence.md)** is cool in easy environments, but doesn't get real world rules.\n\t- *Machine Learning* to yield \"subjective results\"\n- **[Machine Learning](Machine%20Learning.md)** works well with easy data, but is hard to find good features\n\t- *Representation Learning* to learn features\n- **[Representation Learning](Representation%20Learning.md)** leads to too many representations influencing everything, and is hard to find high-level FOV's\n\t- *Deep Learning* to simplify representations","lastmodified":"2022-06-14T23:22:48.537194594Z","tags":null},"/notes/Machine-Learning":{"title":"","content":"# Machine Learning\nPerformance of model ‚àù Representation of data\n### Examples\n- A system that decides the salary of employees\n\t- Would use database with features of employees (hours worked, company title, number of people managed) and their corresponding salaries to create a model\n\t- Then uses model to assign salaries to future employees\n\t- Bad data (where datapoints do not correlate to features) gives bad results\n- **Logistic Regression**: Uses dataset to create mathematical correlations between features and outcomes\n\nTLDR: Use good features for good results\n### Problemo\nBut... it's not that easy. For example, what if you wanted to make an AI identify a car in a photo, but didn't know what features to look for. Looking for wheels, windows, and a \"car shape\" are all very erratic depending on the angle of the photo and the type of car.\n\n**Solution**: Use ML for that too! (aka [Representation Learning](Representation%20Learning.md))","lastmodified":"2022-06-14T23:22:48.537194594Z","tags":null},"/notes/Multi-Layer-Perceptron":{"title":"","content":"# Multi-Layer Perceptron","lastmodified":"2022-06-14T23:22:48.537194594Z","tags":null},"/notes/Representation-Learning":{"title":"","content":"# Representation/Feature Learning\nMethod that uses a set of techniques to let the system automatically find features needed for [Machine Learning](Machine%20Learning.md).\n\n**Example**: [Auto Encoders](Auto%20Encoders.md)\n\n### Factors of Variation\nWhen creating algorithms to learn features, look for **Factors of Variation**, or different sets of features (typically not observable) that affect observable quantities. Going back to the AI car finder example, FOV's would be the brightness of the sun, color, wheels, etc.\n### Overwhelmed...\n- Problem #1: Many FOV's influence every piece of data we can observe (wayyyy too many to be useful at all)\n\t- Solution: Disentangle FOV's, keep only the ones we care about\n- Problem #2: How?\n\t- Solution: [Deep Learning](Deep%20Learning.md)!!","lastmodified":"2022-06-14T23:22:48.537194594Z","tags":null}}