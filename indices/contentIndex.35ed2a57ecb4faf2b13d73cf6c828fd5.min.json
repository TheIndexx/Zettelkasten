{"/":{"title":"Hub of Hubs","content":"# Hello World\nHello! This is my first, and hopefully only, Zettelkasten. It's a tool I plan to use to create hubs of knowledge in my mind so I can gravitate away from the mundane structure of normal note-taking. I have no clue what it will look like at the time you are reading this, but hopefully I can grow it from the dark, empty canvas I see right now to something comprehensive and beautiful. \n\nWith that in mind... welcome! I hope you get the same joy reading this as I do making it.\n\nSome of my interests:\n- [Artificial Intelligence](notes/Artificial%20Intelligence.md)\n- More to come\n\nEnjoy!","lastmodified":"2022-06-17T02:24:02.783381595Z","tags":null},"/notes/Artificial-Intelligence":{"title":"Artificial Intelligence","content":"One of my favorite topics to learn about. The whole concept of machines using the human development of knowledge creates a wonderful connection between biology and technology, 2 unalike fields. There are lots of misconceptions regarding this concept, so the proper definition of AI is simply *the connection between algorithms and datasets to establish patterns.*\n\n## Index\n- [Machine Learning](notes/Machine%20Learning.md)\n- [Representation Learning](notes/Representation%20Learning.md)\n- [Deep Learning](notes/Deep%20Learning.md)\n- [Neural Networks](notes/Neural%20Networks.md)\n\t- [Supervised Learning](notes/Supervised%20Learning.md)\n\t- [Unsupervised Learning](notes/Unsupervised%20Learning.md)\n- [Multi-Layer Perceptron](notes/Multi-Layer%20Perceptron.md)\n- [Logistic Regression](notes/Logistic%20Regression.md) \n\n**Hierarchy**:\nArtificial Intelligence -\u003e [Machine Learning](notes/Machine%20Learning.md) -\u003e [Representation Learning](notes/Representation%20Learning.md) -\u003e [Deep Learning](notes/Deep%20Learning.md) \n## Quick History\nBasically, early successes of AI were in very rudimentary settings (IBM AI winning chess championships). But that didn't really mean much, since the rules of those environments were very simply, not like the real world.\n\nSome people tried to hard code the rules of the world into an AI, called the Knowledge Base Approach -\u003e wasn't effective -\u003e shows that machines need to learn knowledge through patterns from raw data\n## Answer\n[Machine Learning](notes/Machine%20Learning.md)! It successfully yielded \"subjective\" results that were more attuned to reality\n\nFurther reading: Deep Learning textbook","lastmodified":"2022-06-17T02:24:02.783381595Z","tags":null},"/notes/Auto-Encoders":{"title":"Auto Encoders","content":"An important, iconic system in AI classification that involves sending an input through some sort of conversion/encryption tool, then decrypting the result to match the original input as close as possible. The system is fed data of correct conversions, and the system learns the rules of the conversion, and can slowly decipher the code into something similar to the input. Some applications of this technology involves detecting anomalies in datasets, since the expected output would differ highly from the AI output.","lastmodified":"2022-06-17T02:24:02.783381595Z","tags":null},"/notes/Binary-classification":{"title":"Binary Classification","content":"Algorithm where output is either 0 or 1 (could symbolize true/false, yes/no).\n\n### Example\nWe got a 64x64 pixel cat picture. Input is picture, output is either 1 (cat) or 0 (no cat)\n![catpic](/imgs/Pasted%20image%2020220615194949.png)\nIn order to make this more processable for the algorithm, we break the picture into 3 matrices of pixel values for each Red, Green, and Blue pixel. Now, we put it all into the input, X, by putting all the values into a 1 column matrix in order (Red, Green then Blue), and given that it was a 64x64 pixel image, we deduce\n\n$n_{x}(depth of x dimension) = 64*64*3 = 12288$\n\nIn our training set, we have m number of (X, Y) examples, where $x \\in \\mathbb{R}^n_{x}$ and $y \\in (0,1)$.\n\nThus, to efficiently group the X values from all the examples, we will have an X matrix where each column is an input (or 12288 pixel color values) and each row is another example. In Python, X.shape would be a $(n_{x}, m)$ dimensional matrix\n![](/imgs/Pasted%20image%2020220615202710.png)\n\nYou can also stack Y in columns, except Y.shape would be $(1, m)$ since there are only 2 possible outcomes.","lastmodified":"2022-06-17T02:24:02.783381595Z","tags":null},"/notes/Cost-Function":{"title":"Cost Function","content":"## Loss Function\n\n^879b2f\n\nWhen training examples, we need to establish a guideline for error so the learning algorithm knows how far $\\hat{y}$, it's predicted value, was from the real $y$.\n\n$L(\\hat{y}, y) = \\frac{1}{2}(\\hat{y} - y)^2$\n\nThis is a standard squared-error loss function, but for the purpose of logistic regression we will use a Loss Function that it more convex and suitable for [Gradient Descent](notes/Gradient%20Descent.md).\n\n$L(\\hat{y}, y) = -(y \\log{\\hat{y}} + (1 - y) \\log{1-\\hat{y}})$\n\nThis is better for 2 cases.\n1. If $y = 1$: $L(\\hat{y}, y) = -\\log{\\hat{y}}$\n\t- Want $-\\log{\\hat{y}}$ large, want $\\hat{y}$ large\n2. If $y = 0$: $L(\\hat{y}, y) = -\\log{1 - \\hat{y}}$\n\t- Want $- \\log{1 - \\hat{y}}$ large, want $\\hat{y}$ small\n# Cost Function\n$J(w,b) = \\frac{1}{m} \\sum_{i=1}^{m} L(\\hat{y}^i, y^i) = -\\frac{1}{m} \\sum_{i=1}^{m} (y^i \\log{\\hat{y}^i} + (1 - y^i) \\log{1-\\hat{y}^i})$\n\nThe [Loss Function](#^879b2f) is meant for 1 training example, while the Cost Function is applied to a set of parameters to minimize the Loss Function.","lastmodified":"2022-06-17T02:24:02.783381595Z","tags":null},"/notes/Deep-Learning":{"title":"Deep Learning","content":"Representations in terms of more representations (inception type vibe). Often refers to training [Neural Networks](notes/Neural%20Networks.md).\n### Examples\nSay you took a selfie, and we wanted an AI to find you in the picture. To break it down, we will first take the most basic representation, then make another representation in terms of that until we get to you.\n\nPixels -\u003e Edges -\u003e Contours -\u003e Objects -\u003e Person\n\nA fantastic example of this is the [Multi-Layer Perceptron](notes/Multi-Layer%20Perceptron.md), which deserves its own page. But TLDR, it maps input values onto output values, and is a math function composed of many smaller functions, aka representations.\n\nImportant examples:\n\n## Measuring Depth\n1. The number of sequential instructions needed to evaluate the system architecture (the absolute longest path through the flow chart to get from input to output). \n![procedureimage](/imgs/procedureimage.png)\n2. Depth of graph describing relationship between **concepts**. It will likely be smaller than the first method. \n\nComparison of measuring depth with AI observing face of person with a shadow covering half their face.\n\nLayers | Method 1 | Method 2\n------------ | ------------ | ------------\n1 |  1 eye visible | eyes\n2 | portion of face visible | faces\n3 | infers 2nd eye exists | \n\n### Quick summary of process to get to Deep Learning\n- **[Artificial Intelligence](notes/Artificial%20Intelligence.md)** is cool in easy environments, but doesn't get real world rules.\n\t- *Machine Learning* to yield \"subjective results\"\n- **[Machine Learning](notes/Machine%20Learning.md)** works well with easy data, but is hard to find good features\n\t- *Representation Learning* to learn features\n- **[Representation Learning](notes/Representation%20Learning.md)** leads to too many representations influencing everything, and is hard to find high-level FOV's\n\t- *Deep Learning* to simplify representations","lastmodified":"2022-06-17T02:24:02.783381595Z","tags":null},"/notes/Gradient-Descent":{"title":"Gradient Descent","content":"Want to find $w,b$ that minimize $J(w,b)$.  In the image, the gradient is convex, meaning that there is one global minimum instead of several local minimum, and helps us define what cost function to use.\n![](notes/imgs/GradientDescentimg.png)\n\nGradient descent works in iterations; it might start at a random point, then take a step in the direction of steepest descent until it finds the global minimum.\n\nRepeat {\n$w := w - \\alpha \\frac{dJ(w,b)}{dw}$\n$b := b - \\alpha \\frac{dJ(w,b)}{db}$\n}, where $\\alpha$ is the learning rate, and := means \"updates\"\n## Logistic Regression Gradient Descent\n","lastmodified":"2022-06-17T02:24:02.783381595Z","tags":null},"/notes/Logistic-Regression":{"title":"Logistic Regression","content":"#### Text description\nAn algorithm for [Binary classification](notes/Binary%20classification.md) in [Supervised Learning](notes/Supervised%20Learning.md). In statistics terms, it estimates the probability of an event happening or not, like if you vote or didn't vote, given a dataset. There are only 2 possible outcomes, and the predicted Y value only lies between 0 and 1.\n\n#### Math description\nGiven x (features), want $\\hat{y} = P(y=1|x)$\n\nParameters: $w \\in \\mathbb{R}^{n_x}$, $b \\in \\mathbb{R}$\n\nOutput: $\\hat{y} = \\sigma(w^Tx + b)$\n\nA $\\sigma$, or sigmoid function converts whatever number the $(w^Tx + b)$ is to a probability that y = 1, or a range of numbers from 0 to 1. You can look it up if you're interested but I couldn't care less right now.\n![](hub/notes/imgs/sigmoidexplanation.png)\n\nThus, we want to find optimal values of $w$ and $b$ to get $\\hat{y}$ to be as accurate as possible, and we can measure this using a [Cost Function](notes/Cost%20Function.md). One method of training the $w$ parameter is known as [Gradient Descent](notes/Gradient%20Descent.md).\n","lastmodified":"2022-06-17T02:24:02.783381595Z","tags":null},"/notes/Machine-Learning":{"title":"Machine Learning","content":"Performance of model ‚àù Representation of data\n### Examples\n- A system that decides the salary of employees\n\t- Would use database with features of employees (hours worked, company title, number of people managed) and their corresponding salaries to create a model\n\t- Then uses model to assign salaries to future employees\n\t- Bad data (where datapoints do not correlate to features) gives bad results\n- **[Logistic Regression](notes/Logistic%20Regression.md)**: Uses dataset to create mathematical correlations between features and outcomes\n\nTLDR: Use good features for good results\n### Problemo\nBut... it's not that easy. For example, what if you wanted to make an AI identify a car in a photo, but didn't know what features to look for. Looking for wheels, windows, and a \"car shape\" are all very erratic depending on the angle of the photo and the type of car.\n\n**Solution**: Use ML for that too! (aka [Representation Learning](notes/Representation%20Learning.md))","lastmodified":"2022-06-17T02:24:02.783381595Z","tags":null},"/notes/Multi-Layer-Perceptron":{"title":"Multi-Layer Perceptron","content":"","lastmodified":"2022-06-17T02:24:02.783381595Z","tags":null},"/notes/Neural-Networks":{"title":"Neural Networks","content":"## Explanation\nAt its most basic form, a neural network is just a series of nodes that are connected together, inspired by how neurons work in humans.\n\nHere's a very simple example of a single neuron:\n![housingpriceexample](/imgs/Pasted%20image%2020220615182323.png)\n\nThe size of the house, x value, would go into a \"neuron\", where the neuron uses the linear regression function (blue line), and outputs the price, y value. This is a single neuron, and you would get a larger neural network from stacking several of these neurons together.\n\nNow for the neural network:\nSay we have several inputs, or x values: (1) house size, (2) # of bedrooms, (3) zip code, (4) wealth. 1 and 2 would impact **family size**, 3 would impact **walkability**, 3 and 4 would impact **school quality**. Now given family size, walkability and school quality, each with their own linear regression function, we can estimate the price of the house. In practice, you would just give the AI the 4 input values and get a price, but the magic behind the scenes is what really makes it a neural network!\n\nUsing deep learning, we don't have to explicitly say \"Inputs 1 and 2 will correlate with family size\", instead the AI can figure out which inputs will correlate to which nodes. So all we really need to do is give the AI a bunch of X and Y values and it will figure out the relationships itself. A cool effect of this is that every input value impacts every node; in tech terms, we would say the 2 layers are densely connected.\n![neuralnetworkexample](/imgs/Pasted%20image%2020220615184301.png)\n\nNeural networks are most useful in [Supervised Learning](notes/Supervised%20Learning.md), such as the housing example.\n\n### Why are Neural Networks taking off\nScale. Thanks to new advances in technology, humans have much more data on a variety of things than we did before, and normal algorithms ([Logistic Regression](notes/Logistic%20Regression.md), sum) can only understand so much. Deep Learning algorithms are much better at comprehending big data, and we see the highest levels of performance in large neural networks (many parameters, hidden networks) with large amounts of data.\n\nThings to remember:\n- **m** is used to denote amount of data\n- With small training sets, it might be easier to make a normal algorithms than to train a large neural network. The latter only consistently performs well with large training sets.\n\n#### Techniques\nTypically, you might use a *for* loop to iterates through a training set with m examples. \n\nI, an intellectual, would insult your intelligence, and subsequently use forward propogation step and backward propogation step.\n##### Computation graph\nComputation graphs explains the underlying concept behind forward and back propagation. This concept is fairly intuitive to me, but here's some pictures just in case. \n![forwardcomputationgraph](notes/imgs/forwardcomputationgraph.png)\n![computingderivatives](notes/imgs/computingderivatives.png)\n","lastmodified":"2022-06-17T02:24:02.783381595Z","tags":null},"/notes/Representation-Learning":{"title":"Representation Learning","content":"Method that uses a set of techniques to let the system automatically find features needed for [Machine Learning](notes/Machine%20Learning.md).\n\n**Example**: [Auto Encoders](notes/Auto%20Encoders.md)\n\n### Factors of Variation\nWhen creating algorithms to learn features, look for **Factors of Variation**, or different sets of features (typically not observable) that affect observable quantities. Going back to the AI car finder example, FOV's would be the brightness of the sun, color, wheels, etc.\n### Overwhelmed...\n- Problem #1: Many FOV's influence every piece of data we can observe (wayyyy too many to be useful at all)\n\t- Solution: Disentangle FOV's, keep only the ones we care about\n- Problem #2: How?\n\t- Solution: [Deep Learning](notes/Deep%20Learning.md)!!","lastmodified":"2022-06-17T02:24:02.783381595Z","tags":null},"/notes/Supervised-Learning":{"title":"Supervised Learning","content":"**Definition**: It's a learning function that maps an input to an output using example input-output pairs. \n\n### Examples\nLots of money in this field comes from online advertising (ew). These models have gotten really good at predicting what you'll click on given your information. Another example is the housing estimator in [Neural Networks](notes/Neural%20Networks.md). We also see these in photo tagging, speech recognition, machine translation, and autonomous driving. \n\n- Stuctured Data: Databases of data\n\t- Eg. Housing price prediction. One column tells you house size/# of bedrooms. Each of the features has a very well defined meaning\n\t- Most of the current economic value of [Deep Learning](notes/Deep%20Learning.md) is here.\n\n- Unstructured data: Data with no clear meaning\n\t- Eg. Raw audio, images, text. It's harder for computers to make sense of this data, something humans have evolved to do very well.\n\t- Thanks to deep learning, computers are much better for these. ","lastmodified":"2022-06-17T02:24:02.783381595Z","tags":null},"/notes/Unsupervised-Learning":{"title":"Unsupervised Learning","content":"","lastmodified":"2022-06-17T02:24:02.783381595Z","tags":null}}