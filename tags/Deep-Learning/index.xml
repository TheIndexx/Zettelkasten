<?xml version="1.0" encoding="utf-8" standalone="yes"?><rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom"><channel><title>Deep-Learning on</title><link>https://theindexx.github.io/hub/tags/Deep-Learning/</link><description>Recent content in Deep-Learning on</description><generator>Hugo -- gohugo.io</generator><language>en-us</language><atom:link href="https://theindexx.github.io/hub/tags/Deep-Learning/index.xml" rel="self" type="application/rss+xml"/><item><title>Cost Function</title><link>https://theindexx.github.io/hub/ai/Cost-Function/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://theindexx.github.io/hub/ai/Cost-Function/</guid><description>Loss Function When training examples, we need to establish a guideline for error so the learning algorithm knows how far $\hat{y}$, it&amp;rsquo;s predicted value, was from the real $y$.</description></item><item><title>Gradient Descent</title><link>https://theindexx.github.io/hub/ai/Gradient-Descent/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://theindexx.github.io/hub/ai/Gradient-Descent/</guid><description>Want to find $w,b$ that minimize $J(w,b)$. In the image, the gradient is convex, meaning that there is one global minimum instead of several local minimum, and helps us define what cost function to use.</description></item><item><title>Logistic Regression</title><link>https://theindexx.github.io/hub/ai/Logistic-Regression/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://theindexx.github.io/hub/ai/Logistic-Regression/</guid><description>Text description An algorithm for Binary classification in Supervised Learning. In statistics terms, it estimates the probability of an event happening or not, like if you vote or didn&amp;rsquo;t vote, given a dataset.</description></item><item><title>Multi-Layer Perceptron</title><link>https://theindexx.github.io/hub/ai/Multi-Layer-Perceptron/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://theindexx.github.io/hub/ai/Multi-Layer-Perceptron/</guid><description/></item><item><title>Neural Networks</title><link>https://theindexx.github.io/hub/ai/Neural-Networks/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://theindexx.github.io/hub/ai/Neural-Networks/</guid><description>Explanation At its most basic form, a neural network is just a series of nodes that are connected together, inspired by how neurons work in humans.</description></item><item><title>Supervised Learning</title><link>https://theindexx.github.io/hub/ai/Supervised-Learning/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://theindexx.github.io/hub/ai/Supervised-Learning/</guid><description>Definition: It&amp;rsquo;s a learning function that maps an input to an output using example input-output pairs.
Examples Lots of money in this field comes from online advertising (ew).</description></item><item><title>Unsupervised Learning</title><link>https://theindexx.github.io/hub/ai/Unsupervised-Learning/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://theindexx.github.io/hub/ai/Unsupervised-Learning/</guid><description/></item><item><title>Vectorization</title><link>https://theindexx.github.io/hub/ai/Vectorization/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://theindexx.github.io/hub/ai/Vectorization/</guid><description>Example What we need to calculate: $z = w^Tx + b$ Vectorized: $z = np.dot(w, x) + b$ using numpy library Vectorization takes advantages of the SIMD (single instruction multiple data) - or parallelism - instructions on GPU&amp;rsquo;s and CPU&amp;rsquo;s much better.</description></item></channel></rss>